{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Demand Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accurate demand forecasts enable organizations to anticipate demand and consequently allocate the optimal amount of resources to minimize stagnant inventory. In recent years, different methods of machine learning have been applied to solve the demand prediction problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since machine leaning tries to replica what human think and do, the first thing after defining the problem is interviewing with people who do the task manually such as store management to get a better sense of the problem/features that they do care when they make decision. Next step would be understanding the data and gaining insights with visualization. This helps the data scientist get a better sense of current features or any new feature that needs to be built. Then data preparation which includes scaling, normalization, finding outliers, dealing with missing values and feature engineering is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of models is data hungry and any feature could be helpful. Data points that I can think of are clustered into 5 pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item dataset\n",
    "* id\n",
    "* expiry date\n",
    "* perishable\n",
    "* main category\n",
    "* category\n",
    "* shipping cost\n",
    "* unit sale\n",
    "* price\n",
    "* brand\n",
    "* review on social media\n",
    "* sentimental analysis on social media\n",
    "* description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "promotion dataset\n",
    "* start date\n",
    "* end date\n",
    "* item id\n",
    "* before price\n",
    "* after price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transaction dataset \n",
    "* status\n",
    "* status at\n",
    "* store id\n",
    "* item id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calendar dataset\n",
    "* temperature\n",
    "* days\n",
    "* stat holidays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store dataset\n",
    "* id\n",
    "* location, city, province, country\n",
    "* demographics in that local area includes population, education, income,...\n"
 
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of demand prediction methodologies depend on time windows. Therefore, it is a good idea to have features in time windows as well. This is the list of engineered features that could be added to the model:\n",
    "Time window is last 1, 3, 7, 14, 30, 60 days\n",
    "\n",
    "* sales of each item in time windows\n",
    "* mean sales of item/store for each day of week \n",
    "* days since last appearance in store\n",
    "* monthly unit sale for each item\n",
    "* monthly unit sale for each store\n",
    "* mean sale of three most associated item to this item \n",
    "* mean of sale for each item in all store in time windows\n",
    "* mean number of zero sales days during several time windows for item/store pairs\n",
    "* mean sales during several last periods for item/store for each day of month\n",
    "* mean sales during several last periods for item/store main category\n",
    "* mean sales during several last periods for item/store category\n",
    "* mean sales during several last periods for item/store city\n",
    "* mean sales during several last periods for item/store province\n",
    "* mean sales during several last periods for item/store type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all, it is time to build a model. I believe that building this model required tons of data for few years. Also, each item should be treated differently. There are different methodologies that are usually applied to demand prediction. In most cases, the data team starts with owning part of the system and gradually increase that portion. For instance, 70% of decisions will be made by machines and 30% will be made by humans at the beginning and finally machine will take over. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodologies to try out:\n",
    "\n",
    "#### Deep Neural Networks and Recurrent Neural Networds, LSTM\n",
    "LSTM and RNNs to predict next sequence of sale. \n",
    "\n",
    "#### Reinforcement Learning\n",
    "Reinforcement Learning is a domain in artificial intelligence where the models don’t simply make predictions or classifications, but actually act on these predictions. This is done by rewarding and punishing the model for acting incorrectly. In this case, we typically establish punishments for letting an particular inventory item run out of stock, we also punish the model for stock too higher value for too long. For rewards, we primarily focus on ordering items within a safe window before the demand.\n",
    "\n",
    "#### Association Rules for Basket Analysis\n",
    "This method shows which items are bought together. It counts how often theses items occur together. Therefore, it is a good way to have red flag. Or restock related items at the same time. \n",
    "\n",
    "#### Autoregressive Integrated Moving Average (ARIMA). \n",
    "Moving Average Method is one of the oldest and most widely used methods of demand forecasting. In this method, the average sales of the previous 1, 3, 7, 14, 30, 60 days are used as the predictor for the sales of the next day. The predictions are multiplied by a factor that takes care of the difference in sales across the different days of the week. It is simple and gives good accuracy when done on a short-term horizon. However, it is not likely to predict well for a longer-duration span as it is not generalizing the trend mere following the past behavior with auto-regressive components.\n",
    "\n",
    "#### High Performance Gradient Boosting (GBDT, GBRT, GBM or MART) framework based on decision tree \n",
    "It is designed to be run for large data size where it provides the maximum time performance while achieving the same accuracy as other Decision Trees Boosting methods such as XGBoost and pGBRT. It runs by splitting the tree at a leaf rather than at a level.\n",
    "\n",
    "#### Factorization Machines \n",
    "works good with sparse data. Not very fast though but it could uncover the hidden relations between items or different features with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I would do:\n",
    "The model could be a combination of all these models;\n",
    "First moving average, then building a RNN and gradient boosting model that utilize that moving average at different lags. The choice of the method depends on how fast and accurate the model should perform in the business. For instance, arima is the basic model to build. The method is very fast, simple and should give a descent result for short time but not for long time. On the other side, building a neural network requires lots of data and sometimes long training time. However, it could uncover nonlinear relations very well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: \n",
    "\n",
    "At the end the model should be evaluated and deployed if successful. Although iterations are necessary to tweak the parameters and improve the model. Deciding evaluation metric is actually the most important part in real world scenarios. We need to make sure it aligns with the business goal. I posit that the real value of a model to a business is a composite of (1) predictive model accuracy, (2) runtime, (3) scalability and (4) ease of use. Validation could be based on time windows like next 7, 14, 30 days\n",
    "Forecasting models are usually evaluated based on the statistical measures such as Normalized Weighted Mean Squared Logarithmic Error(NWMSLE). Which weight is the weight given to SKU. Perishable items are given higher weights in evaluation\n",
    "We can Use the first year’s data to build the engine by applying machine learning and the second year’s data to fine-tune it by pretending that you don’t know what happened that year. Having more years and more sources of data available, even better, because it will allow us to feed the engine more precisely.\n",
    "Also, a side-by-side test is a good way to develop trust. Compare the results of the analytics model with human forecasters’ results in an area where historical data is available.\n",
    "In any case, the automated system will learn from its experience and human input, tweaking the algorithm and becoming more accurate over time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
